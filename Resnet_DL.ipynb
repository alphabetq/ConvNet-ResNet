{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from PIL import Image\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetFromImages(Dataset):\n",
    "    def __init__(self, table, transforms=None):\n",
    "        # Read the csv file\n",
    "        self.data_info = pd.read_csv(table)\n",
    "        # First column contains the image paths\n",
    "        self.image_arr = np.asarray(self.data_info.iloc[:, 0])\n",
    "        # Second column is the labels\n",
    "        self.label_arr = np.asarray(self.data_info.iloc[:, 1])\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        # Transformation\n",
    "        self.transforms = transforms\n",
    "        # Check number of training label 0 and label 1\n",
    "        print(len(self.data_info[self.data_info[\"label\"]==1]))\n",
    "        print(len(self.data_info[self.data_info[\"label\"]==0]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        single_image_name = self.image_arr[index]\n",
    "        # Open image\n",
    "#         img_as_img = Image.open(single_image_name).convert('RGB')\n",
    "        img_as_img = Image.open(single_image_name).convert('1')\n",
    "\n",
    "        # Do some operation on image and Transform image to tensor\n",
    "        img_as_img = img_as_img.resize((224,224))\n",
    "        if self.transforms is not None:\n",
    "            img_as_tensor = self.transforms(img_as_img)\n",
    "\n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        self.single_image_label = self.label_arr[index]\n",
    "        \n",
    "        return (img_as_tensor, self.single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Aug = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.RandomApply([\n",
    "                                    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2), \n",
    "                                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                    transforms.RandomVerticalFlip(p=0.5),\n",
    "                                    transforms.RandomGrayscale(p=0.1),\n",
    "                                    transforms.RandomAffine(10, translate=(0.2, 0.2), fillcolor=0)], p=0.5), \n",
    "                                transforms.ToTensor(), \n",
    "                               ])\n",
    "test_Aug = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "train_set=CustomDatasetFromImages('C:/train.csv', transforms=train_Aug)\n",
    "test_set=CustomDatasetFromImages('C:/test.csv', transforms=test_Aug)\n",
    "trainloader= DataLoader(train_set, batch_size=32, num_workers=2, shuffle=True) \n",
    "testloader= DataLoader(test_set, batch_size=32, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainloader))\n",
    "print(len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# model = models.resnet18(pretrained=True)\n",
    "model = models.resnet34(pretrained=True)\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# model = torchvision.models.resnet50()\n",
    "# model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "#for resnet50, remove nn.Linear(512, 128)\n",
    "model.fc = nn.Sequential(nn.Linear(512, 128),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(128, 2),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 18\n",
    "epochs = 100\n",
    "best_acc = 0.00\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "    _loss = []\n",
    "\n",
    "    model.train(True)\n",
    "    for data, label in trainloader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        _loss.append(loss.detach().item())\n",
    "        train_predictions += torch.argmax(output, dim=1).detach().cpu().numpy().tolist()\n",
    "        train_labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "    print(\"learning rate - \"+str(get_lr(optimizer)))\n",
    "    scheduler.step()\n",
    "\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    _test_loss=[]\n",
    "    model.train(False)\n",
    "    if (epoch+ 1) % 5 == 0:\n",
    "        for data, label in testloader:\n",
    "            data, label = data.to(device), label.to(device) \n",
    "            output = model(data)\n",
    "            test_loss = criterion(output, label)\n",
    "            _test_loss.append(test_loss.detach().item())\n",
    "            test_predictions += torch.argmax(output, dim=1).detach().cpu().numpy().tolist()\n",
    "            test_labels += label.detach().cpu().numpy().tolist()\n",
    "            test_acc=accuracy_score(test_labels, test_predictions)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {np.mean(_loss):.8f} \"\n",
    "                      f\"Test loss: {np.mean(_test_loss):.8f} \"\n",
    "                      f\"Train accuracy: {accuracy_score(train_labels, train_predictions):.8f} \"\n",
    "                      f\"Test accuracy: {accuracy_score(test_labels, test_predictions):.8f} \")\n",
    "        if (epoch+ 1) % 25 == 0:\n",
    "            print(classification_report(test_labels, test_predictions))\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {np.mean(_loss):.8f} \"\n",
    "                      f\"Train accuracy: {accuracy_score(train_labels, train_predictions):.8f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 34\n",
    "epochs = 100\n",
    "best_acc = 0.00\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "    _loss = []\n",
    "\n",
    "    model.train(True)\n",
    "    for data, label in trainloader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        _loss.append(loss.detach().item())\n",
    "        train_predictions += torch.argmax(output, dim=1).detach().cpu().numpy().tolist()\n",
    "        train_labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "    print(\"learning rate - \"+str(get_lr(optimizer)))\n",
    "    scheduler.step()\n",
    "\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    _test_loss=[]\n",
    "    model.train(False)\n",
    "    if (epoch+ 1) % 5 == 0:\n",
    "        for data, label in testloader:\n",
    "            data, label = data.to(device), label.to(device) \n",
    "            output = model(data)\n",
    "            test_loss = criterion(output, label)\n",
    "            _test_loss.append(test_loss.detach().item())\n",
    "            test_predictions += torch.argmax(output, dim=1).detach().cpu().numpy().tolist()\n",
    "            test_labels += label.detach().cpu().numpy().tolist()\n",
    "            test_acc=accuracy_score(test_labels, test_predictions)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {np.mean(_loss):.8f} \"\n",
    "                      f\"Test loss: {np.mean(_test_loss):.8f} \"\n",
    "                      f\"Train accuracy: {accuracy_score(train_labels, train_predictions):.8f} \"\n",
    "                      f\"Test accuracy: {accuracy_score(test_labels, test_predictions):.8f} \")\n",
    "        if (epoch+ 1) % 25 == 0:\n",
    "            print(classification_report(test_labels, test_predictions))\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {np.mean(_loss):.8f} \"\n",
    "                      f\"Train accuracy: {accuracy_score(train_labels, train_predictions):.8f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 50\n",
    "epochs = 100\n",
    "best_acc = 0.00\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "    _loss = []\n",
    "\n",
    "    model.train(True)\n",
    "    for data, label in trainloader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        _loss.append(loss.detach().item())\n",
    "        train_predictions += torch.argmax(output, dim=1).detach().cpu().numpy().tolist()\n",
    "        train_labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "    print(\"learning rate - \"+str(get_lr(optimizer)))\n",
    "    scheduler.step()\n",
    "\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    _test_loss=[]\n",
    "    model.train(False)\n",
    "    if (epoch+ 1) % 5 == 0:\n",
    "        for data, label in testloader:\n",
    "            data, label = data.to(device), label.to(device) \n",
    "            output = model(data)\n",
    "            test_loss = criterion(output, label)\n",
    "            _test_loss.append(test_loss.detach().item())\n",
    "            test_predictions += torch.argmax(output, dim=1).detach().cpu().numpy().tolist()\n",
    "            test_labels += label.detach().cpu().numpy().tolist()\n",
    "            test_acc=accuracy_score(test_labels, test_predictions)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {np.mean(_loss):.8f} \"\n",
    "                      f\"Test loss: {np.mean(_test_loss):.8f} \"\n",
    "                      f\"Train accuracy: {accuracy_score(train_labels, train_predictions):.8f} \"\n",
    "                      f\"Test accuracy: {accuracy_score(test_labels, test_predictions):.8f} \")\n",
    "        if (epoch+ 1) % 25 == 0:\n",
    "            print(classification_report(test_labels, test_predictions))\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {np.mean(_loss):.8f} \"\n",
    "                      f\"Train accuracy: {accuracy_score(train_labels, train_predictions):.8f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet 50\n",
    "# Upsampling the minority with two times more\n",
    "epochs = 100\n",
    "best_acc = 0.00\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "    _loss = []\n",
    "\n",
    "    model.train(True)\n",
    "    for data, label in trainloader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        _loss.append(loss.detach().item())\n",
    "        train_predictions += torch.argmax(output, dim=1).detach().cpu().numpy().tolist()\n",
    "        train_labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "    print(\"learning rate - \"+str(get_lr(optimizer)))\n",
    "    scheduler.step()\n",
    "\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    _test_loss=[]\n",
    "    model.train(False)\n",
    "    if (epoch+ 1) % 5 == 0:\n",
    "        for data, label in testloader:\n",
    "            data, label = data.to(device), label.to(device) \n",
    "            output = model(data)\n",
    "            test_loss = criterion(output, label)\n",
    "            _test_loss.append(test_loss.detach().item())\n",
    "            test_predictions += torch.argmax(output, dim=1).detach().cpu().numpy().tolist()\n",
    "            test_labels += label.detach().cpu().numpy().tolist()\n",
    "            test_acc=accuracy_score(test_labels, test_predictions)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {np.mean(_loss):.8f} \"\n",
    "                      f\"Test loss: {np.mean(_test_loss):.8f} \"\n",
    "                      f\"Train accuracy: {accuracy_score(train_labels, train_predictions):.8f} \"\n",
    "                      f\"Test accuracy: {accuracy_score(test_labels, test_predictions):.8f} \")\n",
    "        if (epoch+ 1) % 25 == 0:\n",
    "            print(classification_report(test_labels, test_predictions))\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {np.mean(_loss):.8f} \"\n",
    "                      f\"Train accuracy: {accuracy_score(train_labels, train_predictions):.8f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(test_pred_saved).shape)\n",
    "print(np.array(test_actual_saved).shape)\n",
    "print(np.array(test_img_saved).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # actual bad, predicted bad\n",
    "\n",
    "image_1=[]\n",
    "new_act=[]\n",
    "new_pred=[]\n",
    "for i in range (len(test_img_saved)):\n",
    "    for j in range(len(test_img_saved[0])):\n",
    "        try:\n",
    "            if test_actual_saved[i*32+j] == test_pred_saved[i*32+j]:\n",
    "                if test_actual_saved[i*32+j] == 0:\n",
    "                    image_1.append(test_img_saved[i][j])\n",
    "                    new_act.append(test_actual_saved[i*32+j])\n",
    "                    new_pred.append(test_pred_saved[i*32+j])\n",
    "        except:\n",
    "            None\n",
    "\n",
    "bad = 0\n",
    "for num in test_labels:\n",
    "    if num == 0:\n",
    "        bad += 1\n",
    "\n",
    "predict = len(image_1) * (1 - accuracy_score(test_labels, test_predictions))\n",
    "if bad != 0:\n",
    "    divide = len(image_1) / bad * 100\n",
    "else:\n",
    "    divide = 0\n",
    "print('Test images: ' + str(len(test_set)))\n",
    "print('Bad images: ' + str(bad))\n",
    "print('Actual bad, predicted bad: ' + str(len(image_1)))\n",
    "print('Predicted percentage: ' + str(\"{:.4f}\".format((accuracy_score(test_labels, test_predictions) * 100))) + '%')\n",
    "print('Actual percentage: ' + str((\"{:.4f}\".format(divide))) + '%')\n",
    "print('Predicted false calls: ' + str((\"{:.4f}\".format(predict))))\n",
    "print('Actual false calls: ' + str(bad - len(image_1)))\n",
    "\n",
    "diff = divide - (accuracy_score(test_labels, test_predictions) * 100)\n",
    "if -1 < diff:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'green') + '%')\n",
    "\n",
    "elif -3 < diff <= -1:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'yellow') + '%')\n",
    "    \n",
    "elif diff <= -3:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'red') + '%')\n",
    "\n",
    "if predict > (bad - len(image_1)):\n",
    "    diff = predict - (bad - len(image_1))\n",
    "    print('False call difference: ' + str((\"{:.4f}\".format(diff))))\n",
    "    \n",
    "else:\n",
    "    diff = (bad - len(image_1)) - predict\n",
    "    print('False call difference: ' + str((\"{:.4f}\".format(diff))))\n",
    "\n",
    "#inv_normalize = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "#                                                          std = [ 1/0.5, 1/0.5, 1/0.5]),\n",
    "#                                    transforms.Normalize(mean = [ -0.5, -0.5, -0.5],\n",
    "#                                                         std = [ 1., 1., 1. ]),\n",
    "#                                   ])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(15,20*len(image_1)/5))\n",
    "k=0\n",
    "for i in range(len(image_1)):\n",
    "    #inv_tensor = inv_normalize(image_1[i])\n",
    "    #temp=inv_tensor.cpu().detach().numpy()\n",
    "    temp=image_1[i].cpu().detach().numpy()\n",
    "    new_image=temp.transpose((1,2,0))\n",
    "    fig.add_subplot(len(image_1), 5, i+1)\n",
    "    plt.title(\"Actual:\"+str(new_act[i])+\" Predicted:\"+ str(new_pred[i]))\n",
    "    plt.imshow(new_image)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual good, predicted good\n",
    "\n",
    "image_1=[]\n",
    "new_act=[]\n",
    "new_pred=[]\n",
    "for i in range (len(test_img_saved)):\n",
    "    for j in range(len(test_img_saved[0])):\n",
    "        try:\n",
    "            if test_actual_saved[i*32+j] == test_pred_saved[i*32+j]:\n",
    "                if test_actual_saved[i*32+j] == 1:\n",
    "                    image_1.append(test_img_saved[i][j])\n",
    "                    new_act.append(test_actual_saved[i*32+j])\n",
    "                    new_pred.append(test_pred_saved[i*32+j])\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "good = 0\n",
    "for num in test_labels:\n",
    "    if num == 1:\n",
    "        good += 1\n",
    "\n",
    "predict = len(image_1) * (1 - accuracy_score(test_labels, test_predictions))\n",
    "if good != 0:\n",
    "    divide = len(image_1) / good * 100\n",
    "else:\n",
    "    divide = 0\n",
    "print('Test images: ' + str(len(test_set)))\n",
    "print('Good images: ' + str(good))\n",
    "print('Actual good, predicted good: ' + str(len(image_1)))\n",
    "print('Predicted percentage: ' + str(\"{:.4f}\".format((accuracy_score(test_labels, test_predictions) * 100))) + '%')\n",
    "print('Actual percentage: ' + str((\"{:.4f}\".format(divide))) + '%')\n",
    "print('Predicted false calls: ' + str((\"{:.4f}\".format(predict))))\n",
    "print('Actual false calls: ' + str(good - len(image_1)))\n",
    "\n",
    "diff = divide - (accuracy_score(test_labels, test_predictions) * 100)\n",
    "if -1 < diff:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'green') + '%')\n",
    "\n",
    "elif -3 < diff <= -1:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'yellow') + '%')\n",
    "    \n",
    "elif diff <= -3:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'red') + '%')\n",
    "\n",
    "if predict > (good - len(image_1)):\n",
    "    diff = predict - (good - len(image_1))\n",
    "    print('False call difference: ' + str((\"{:.4f}\".format(diff))))\n",
    "    \n",
    "else:\n",
    "    diff = (good - len(image_1)) - predict\n",
    "    print('False call difference: ' + str((\"{:.4f}\".format(diff))))\n",
    "\n",
    "#inv_normalize = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "#                                                          std = [ 1/0.5, 1/0.5, 1/0.5]),\n",
    "#                                    transforms.Normalize(mean = [ -0.5, -0.5, -0.5],\n",
    "#                                                         std = [ 1., 1., 1. ]),\n",
    "#                                   ])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(15,20*len(image_1)/5))\n",
    "k=0\n",
    "for i in range(len(image_1)):\n",
    "    #inv_tensor = inv_normalize(image_1[i])\n",
    "    #temp=inv_tensor.cpu().detach().numpy()\n",
    "    temp=image_1[i].cpu().detach().numpy()\n",
    "    new_image=temp.transpose((1,2,0))\n",
    "    fig.add_subplot(len(image_1), 5, i+1)\n",
    "    plt.title(\"Actual:\"+str(new_act[i])+\" Predicted:\"+ str(new_pred[i]))\n",
    "    plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual bad, predicted good\n",
    "\n",
    "image_1=[]\n",
    "new_act=[]\n",
    "new_pred=[]\n",
    "for i in range (len(test_img_saved)):\n",
    "    for j in range(len(test_img_saved[0])):\n",
    "        try:\n",
    "            if test_actual_saved[i*32+j] != test_pred_saved[i*32+j]:\n",
    "                if test_actual_saved[i*32+j] == 0:\n",
    "                    image_1.append(test_img_saved[i][j])\n",
    "                    new_act.append(test_actual_saved[i*32+j])\n",
    "                    new_pred.append(test_pred_saved[i*32+j])\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "predict = (len(test_set)) * (1 - (accuracy_score(test_labels, test_predictions)))\n",
    "divide = (len(test_set) - len(image_1)) / len(test_set) * 100\n",
    "print('Test images: ' + str(len(test_set)))\n",
    "print('Actual bad, predicted good: ' + str(len(image_1)))\n",
    "print('Predicted percentage: ' + str(\"{:.4f}\".format((accuracy_score(test_labels, test_predictions) * 100))) + '%')\n",
    "print('Actual percentage: ' + str((\"{:.4f}\".format(divide))) + '%')\n",
    "print('Predicted false calls: ' + str((\"{:.4f}\".format(predict))))\n",
    "print('Actual false calls: ' + str(\"{:.4f}\".format((len(test_set) * (100 - divide) / 100))))\n",
    "\n",
    "diff = divide - (accuracy_score(test_labels, test_predictions) * 100)\n",
    "if -1 < diff:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'green') + '%')\n",
    "\n",
    "elif -3 < diff <= -1:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'yellow') + '%')\n",
    "    \n",
    "elif diff <= -3:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'red') + '%')\n",
    "\n",
    "if predict > len(image_1):\n",
    "    diff = predict - len(image_1)\n",
    "    print('False call difference: ' + str((\"{:.4f}\".format(diff))))\n",
    "    \n",
    "else:\n",
    "    diff = len(image_1) - predict\n",
    "    print('False call difference: ' + str((\"{:.4f}\".format(diff))))\n",
    "\n",
    "#inv_normalize = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "#                                                          std = [ 1/0.5, 1/0.5, 1/0.5]),\n",
    "#                                    transforms.Normalize(mean = [ -0.5, -0.5, -0.5],\n",
    "#                                                         std = [ 1., 1., 1. ]),\n",
    "#                                   ])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(15,20*len(image_1)/5))\n",
    "k=0\n",
    "for i in range(len(image_1)):\n",
    "    #inv_tensor = inv_normalize(image_1[i])\n",
    "    #temp=inv_tensor.cpu().detach().numpy()\n",
    "    temp=image_1[i].cpu().detach().numpy()\n",
    "    new_image=temp.transpose((1,2,0))\n",
    "    fig.add_subplot(len(image_1), 5, i+1)\n",
    "    plt.title(\"Actual:\"+str(new_act[i])+\" Predicted:\"+ str(new_pred[i]))\n",
    "    plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual good, predicted bad\n",
    "\n",
    "image_1=[]\n",
    "new_act=[]\n",
    "new_pred=[]\n",
    "for i in range (len(test_img_saved)):\n",
    "    for j in range(len(test_img_saved[0])):\n",
    "        try:\n",
    "            if test_actual_saved[i*32+j] != test_pred_saved[i*32+j]:\n",
    "                if test_actual_saved[i*32+j] == 1:\n",
    "                    image_1.append(test_img_saved[i][j])\n",
    "                    new_act.append(test_actual_saved[i*32+j])\n",
    "                    new_pred.append(test_pred_saved[i*32+j])\n",
    "        except:\n",
    "            None\n",
    "\n",
    "predict = (len(test_set)) * (1 - (accuracy_score(test_labels, test_predictions)))\n",
    "divide = (len(test_set) - len(image_1)) / len(test_set) * 100\n",
    "print('Test images: ' + str(len(test_set)))\n",
    "print('Actual good, predicted bad: ' + str(len(image_1)))\n",
    "print('Predicted percentage: ' + str(\"{:.4f}\".format((accuracy_score(test_labels, test_predictions) * 100))) + '%')\n",
    "print('Actual percentage: ' + str((\"{:.4f}\".format(divide))) + '%')\n",
    "print('Predicted false calls: ' + str((\"{:.4f}\".format(predict))))\n",
    "print('Actual false calls: ' + str(\"{:.4f}\".format((len(test_set) * (100 - divide) / 100))))\n",
    "\n",
    "diff = divide - (accuracy_score(test_labels, test_predictions) * 100)\n",
    "if -1 < diff:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'green') + '%')\n",
    "\n",
    "elif -3 < diff <= -1:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'yellow') + '%')\n",
    "    \n",
    "elif diff <= -3:\n",
    "    print('Percentage difference: ' + colored(str((\"{:+.4f}\".format(diff))), 'red') + '%')\n",
    "    \n",
    "if predict > len(image_1):\n",
    "    diff = predict - len(image_1)\n",
    "    print('False call difference: ' + str((\"{:.4f}\".format(diff))))\n",
    "    \n",
    "else:\n",
    "    diff = len(image_1) - predict\n",
    "    print('False call difference: ' + str((\"{:.4f}\".format(diff))))\n",
    "\n",
    "\n",
    "#inv_normalize = transforms.Compose([transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "#                                                          std = [ 1/0.5, 1/0.5, 1/0.5]),\n",
    "#                                    transforms.Normalize(mean = [ -0.5, -0.5, -0.5],\n",
    "#                                                         std = [ 1., 1., 1. ]),\n",
    "#                                   ])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(15,20*len(image_1)/5))\n",
    "k=0\n",
    "for i in range(len(image_1)):\n",
    "    #inv_tensor = inv_normalize(image_1[i])\n",
    "    #temp=inv_tensor.cpu().detach().numpy()\n",
    "    temp=image_1[i].cpu().detach().numpy()\n",
    "    new_image=temp.transpose((1,2,0))\n",
    "    fig.add_subplot(len(image_1), 5, i+1)\n",
    "    plt.title(\"Actual:\"+str(new_act[i])+\" Predicted:\"+ str(new_pred[i]))\n",
    "    if i <= 25:\n",
    "        plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
